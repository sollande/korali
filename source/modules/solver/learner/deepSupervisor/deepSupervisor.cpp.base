#include "engine.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepSupervisor/deepSupervisor.hpp"
#include "sample/sample.hpp"
#include <cstdio>
#include <iostream>
#include <omp.h>
#include <string>
#include <signal.h>

__startNamespace__;

void __className__::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

  // Fixing termination criteria for testing mode
  if (_mode == "Testing") _maxGenerations = _k->_currentGeneration + 1;

  // Don't reinitialize if experiment was already initialized
  if (_k->_isInitialized == true) return;

  // Check whether the minibatch size (N) can be divided by the requested concurrency
  if (_problem->_trainingBatchSize % _batchConcurrency > 0) KORALI_LOG_ERROR("The training concurrency requested (%lu) does not divide the training mini batch size (%lu) perfectly.", _batchConcurrency, _problem->_trainingBatchSize);

  // Check whether the minibatch size (N) can be divided by the requested concurrency
  if (_problem->_testingBatchSize % _batchConcurrency > 0) KORALI_LOG_ERROR("The Testing concurrency requested (%lu) does not divide the training mini batch size (%lu) perfectly.", _batchConcurrency, _problem->_testingBatchSize);

  // Determining batch sizes
  std::vector<size_t> batchSizes = {
  _problem->_trainingBatchSize,
  _problem->_validationBatchSize,
  _problem->_testingBatchSize
};

  // If parallelizing training, we need to support the split batch size
  if (_batchConcurrency > 1) batchSizes.push_back(_problem->_trainingBatchSize / _batchConcurrency);
  if (_batchConcurrency > 1) batchSizes.push_back(_problem->_testingBatchSize / _batchConcurrency);

  /*****************************************************************
   * Setting up Neural Networks
   *****************************************************************/

  // Configuring neural network's inputs
  knlohmann::json neuralNetworkConfig;
  neuralNetworkConfig["Type"] = "Neural Network";
  neuralNetworkConfig["Engine"] = _neuralNetworkEngine;
  neuralNetworkConfig["Timestep Count"] = _problem->_maxTimesteps;

  // Iterator for the current layer id
  size_t curLayer = 0;

  // Setting the number of input layer nodes as number of input vector size
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Input";
  neuralNetworkConfig["Layers"][curLayer]["Output Channels"] = _problem->_inputSize;
  curLayer++;

  // Adding user-defined hidden layers
  for (size_t i = 0; i < _neuralNetworkHiddenLayers.size(); i++)
  {
    neuralNetworkConfig["Layers"][curLayer]["Weight Scaling"] = _outputWeightsScaling;
    neuralNetworkConfig["Layers"][curLayer] = _neuralNetworkHiddenLayers[i];
    curLayer++;
  }

  // Adding linear transformation layer to convert hidden state to match output channels
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Linear";
  neuralNetworkConfig["Layers"][curLayer]["Output Channels"] = _problem->_solutionSize;
  neuralNetworkConfig["Layers"][curLayer]["Weight Scaling"] = _outputWeightsScaling;
  curLayer++;

  // Applying a user-defined pre-activation function
  if (_neuralNetworkOutputActivation != "Identity")
  {
    neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Activation";
    neuralNetworkConfig["Layers"][curLayer]["Function"] = _neuralNetworkOutputActivation;
    curLayer++;
  }

  // Applying output layer configuration
  neuralNetworkConfig["Layers"][curLayer] = _neuralNetworkOutputLayer;
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Output";

  // Instancing training neural network
  auto trainingNeuralNetworkConfig = neuralNetworkConfig;
  trainingNeuralNetworkConfig["Batch Sizes"] = batchSizes;
  trainingNeuralNetworkConfig["Mode"] = "Training";
  _neuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(trainingNeuralNetworkConfig, _k));
  _neuralNetwork->applyModuleDefaults(trainingNeuralNetworkConfig);
  _neuralNetwork->setConfiguration(trainingNeuralNetworkConfig);
  _neuralNetwork->initialize();

  /*****************************************************************
   * Initializing NN hyperparameters
   *****************************************************************/

  // If the hyperparameters have not been specified, produce new initial ones
  if (_hyperparameters.size() == 0) _hyperparameters = _neuralNetwork->generateInitialHyperparameters();

  /*****************************************************************
   * Setting up weight and bias optimization experiment
   *****************************************************************/

  if (_neuralNetworkOptimizer == "Adam") _optimizer = new korali::fAdam(_hyperparameters.size());
  if (_neuralNetworkOptimizer == "AdaBelief") _optimizer = new korali::fAdaBelief(_hyperparameters.size());
  if (_neuralNetworkOptimizer == "MADGRAD") _optimizer = new korali::fMadGrad(_hyperparameters.size());
  if (_neuralNetworkOptimizer == "RMSProp") _optimizer = new korali::fRMSProp(_hyperparameters.size());
  if (_neuralNetworkOptimizer == "Adagrad") _optimizer = new korali::fAdagrad(_hyperparameters.size());

  // Setting hyperparameter structures in the neural network and optmizer
  setHyperparameters(_hyperparameters);

  // Resetting Optimizer
  _optimizer->reset();

  // Setting current loss
  _currentTrainingLoss = 0.0f;
  _currentValidationLoss = 0.0f;
  // Getting and setting the validation set
  _validationSetSize = _problem->_validationSetData.size();
  if(_validationSetData.size()){
    _validationSetData = flatten(_problem->_validationSetData);
    _validationSetSolution = flatten(_problem->_validationSetSolution);
  }

}

void __className__::runGeneration()
{
  if (_mode == "Training") runEpoch();
  if (_mode == "Testing") runTestingGeneration();
}


void __className__::runTestingGeneration()
{

  // Check whether training concurrency exceeds the number of workers
  if (_batchConcurrency > _k->_engine->_conduit->getWorkerCount()) KORALI_LOG_ERROR("The batch concurrency requested (%lu) exceeds the number of Korali workers defined in the conduit type/configuration (%lu).", _batchConcurrency, _k->_engine->_conduit->getWorkerCount());

  // Checking that incoming data has a correct format
  if (_problem->_testingBatchSize != _problem->_inputData.size())
    KORALI_LOG_ERROR("Testing Batch size %lu different than that of input data (%lu).\n", _problem->_testingBatchSize, _problem->_inputData.size());

  // In case we run Mean Squared Error with concurrency, distribute the work among samples
  if (_batchConcurrency > 1)
  {
    // Calculating per worker dimensions
    size_t NW = _problem->_testingBatchSize / _batchConcurrency;
    size_t T = _problem->_inputData[0].size();
    size_t IC = _problem->_inputData[0][0].size();

    // Getting current NN hyperparameters
    const auto nnHyperparameters = _neuralNetwork->getHyperparameters();

    // Sending input to workers for parallel processing
    std::vector<Sample> samples(_batchConcurrency);
#pragma omp parallel for
    for (size_t sId = 0; sId < _batchConcurrency; sId++)
    {
      // Carving part of the batch data that corresponds to this sample
      // NW = N/_batchConcurrency
      // inputData[NxTxIC]
      auto workerInputDataFlat = std::vector<float>(NW * IC * T);
      for (size_t i = 0; i < NW; i++)
        for (size_t j = 0; j < T; j++)
          for (size_t k = 0; k < IC; k++)
            workerInputDataFlat[i * T * IC + j * IC + k] = _problem->_inputData[sId * NW + i][j][k];

      // Setting up sample
      samples[sId]["Sample Id"] = sId;
      samples[sId]["Module"] = "Solver";
      samples[sId]["Operation"] = "Run Evaluation On Worker";
      samples[sId]["Input Data"] = workerInputDataFlat;
      samples[sId]["Input Dims"] = std::vector<size_t>({NW, T, IC});
      samples[sId]["Hyperparameters"] = nnHyperparameters;
    }

    // Launching samples
    for (size_t i = 0; i < _batchConcurrency; i++) KORALI_START(samples[i]);

    // Waiting for samples to finish
    KORALI_WAITALL(samples);

    // Assembling hyperparameters and the total mean squared loss
    _evaluation.clear();
    for (size_t i = 0; i < _batchConcurrency; i++)
    {
      const auto workerEvaluations = KORALI_GET(std::vector<std::vector<float>>, samples[i], "Evaluation");
      _evaluation.insert(_evaluation.end(), workerEvaluations.begin(), workerEvaluations.end());
    }
  }

  // If we use an MSE loss function, we need to update the gradient vector with its difference with each of batch's last timestep of the NN output
  if (_batchConcurrency == 1)
  {
    // Getting a reference to the neural network output
    _evaluation = getEvaluation(_problem->_inputData);
  }
}

void __className__::runTrainingGeneration()
{
  // Grabbing batch size
  const size_t N = _problem->_trainingBatchSize;

  // Check whether training concurrency exceeds the number of workers
  if (_batchConcurrency > _k->_engine->_conduit->getWorkerCount()) KORALI_LOG_ERROR("The batch concurrency requested (%lu) exceeds the number of Korali workers defined in the conduit type/configuration (%lu).", _batchConcurrency, _k->_engine->_conduit->getWorkerCount());

  // Updating solver's learning rate, if changed
  _optimizer->_eta = _learningRate;

  // Checking that incoming data has a correct format
  _problem->verifyData();

  // Hyperparameter gradient storage
  std::vector<float> nnHyperparameterGradients;

  // In case we run Mean Squared Error with concurrency, distribute the work among samples
  if (_lossFunction == "Mean Squared Error" && _batchConcurrency > 1)
  {
    // Calculating per worker dimensions
    size_t NW = _problem->_trainingBatchSize / _batchConcurrency;
    size_t T = _problem->_inputData[0].size();
    size_t IC = _problem->_inputData[0][0].size();
    size_t OC = _problem->_solutionData[0].size();

    // Getting current NN hyperparameters
    const auto nnHyperparameters = _neuralNetwork->getHyperparameters();

    // Sending input to workers for parallel processing
    std::vector<Sample> samples(_batchConcurrency);
#pragma omp parallel for
    for (size_t sId = 0; sId < _batchConcurrency; sId++)
    {
      // Carving part of the batch data that corresponds to this sample
      auto workerInputDataFlat = std::vector<float>(NW * IC * T);
      for (size_t i = 0; i < NW; i++)
        for (size_t j = 0; j < T; j++)
          for (size_t k = 0; k < IC; k++)
            workerInputDataFlat[i * T * IC + j * IC + k] = _problem->_inputData[sId * NW + i][j][k];

      auto workerSolutionDataFlat = std::vector<float>(NW * IC * T);
      for (size_t i = 0; i < NW; i++)
        for (size_t j = 0; j < OC; j++)
          workerSolutionDataFlat[i * OC + j] = _problem->_solutionData[sId * NW + i][j];

      // Setting up sample
      samples[sId]["Sample Id"] = sId;
      samples[sId]["Module"] = "Solver";
      samples[sId]["Operation"] = "Run Training On Worker";
      samples[sId]["Input Data"] = workerInputDataFlat;
      samples[sId]["Input Dims"] = std::vector<size_t>({NW, T, IC});
      samples[sId]["Solution Data"] = workerSolutionDataFlat;
      samples[sId]["Solution Dims"] = std::vector<size_t>({NW, OC});
      samples[sId]["Hyperparameters"] = nnHyperparameters;
    }

    // Launching samples
    for (size_t i = 0; i < _batchConcurrency; i++) KORALI_START(samples[i]);

    // Waiting for samples to finish
    KORALI_WAITALL(samples);

    // Assembling hyperparameters and the total mean squared loss
    _currentTrainingLoss = 0.0f;
    nnHyperparameterGradients = std::vector<float>(_neuralNetwork->_hyperparameterCount, 0.0f);
    for (size_t i = 0; i < _batchConcurrency; i++)
    {
      _currentTrainingLoss += KORALI_GET(float, samples[i], "Squared Loss");
      const auto workerGradients = KORALI_GET(std::vector<float>, samples[i], "Hyperparameter Gradients");
      for (size_t i = 0; i < workerGradients.size(); i++) nnHyperparameterGradients[i] += workerGradients[i];
    }
    _currentTrainingLoss = _currentTrainingLoss / ((float)N * 2.0f);
  }

  // If we use an MSE loss function, we need to update the gradient vector with its difference with each of batch's last timestep of the NN output
  if (_lossFunction == "Mean Squared Error" && _batchConcurrency == 1)
  {
    // Grabbing constants
    const size_t OC = _problem->_solutionSize;

    // Making a copy of the solution data for MSE calculation
    auto MSEVector = _problem->_solutionData;

    // Getting a reference to the neural network output
    const auto &results = getEvaluation(_problem->_inputData);

// Calculating gradients via the loss function
#pragma omp parallel for simd
    for (size_t b = 0; b < N; b++)
      for (size_t i = 0; i < OC; i++)
        MSEVector[b][i] = MSEVector[b][i] - results[b][i];

    for (size_t b = 0; b < N; b++)
      for (size_t i = 0; i < OC; i++)
        _currentTrainingLoss += MSEVector[b][i] * MSEVector[b][i];
    _currentTrainingLoss = _currentTrainingLoss / ((float)N * 2.0f);

    // Running back propagation on the MSE vector
    nnHyperparameterGradients = backwardGradients(MSEVector);
  }

  // If the solution represents the gradients, just pass them on
  if (_lossFunction == "Direct Gradient") nnHyperparameterGradients = backwardGradients(_problem->_solutionData);

  // Passing hyperparameter gradients through a gradient descent update
  _optimizer->processResult(0.0f, nnHyperparameterGradients);

  // Getting new set of hyperparameters from the gradient descent algorithm
  _neuralNetwork->setHyperparameters(_optimizer->_currentValue);
}

void __className__::runEpoch()
{
  // printf("runTrainingGeneration\n"); fflush(stdout);
  // Check whether training concurrency exceeds the number of workers
  if (_batchConcurrency > _k->_engine->_conduit->getWorkerCount()) KORALI_LOG_ERROR("The batch concurrency requested (%lu) exceeds the number of Korali workers defined in the conduit type/configuration (%lu).", _batchConcurrency, _k->_engine->_conduit->getWorkerCount());
  // printf("Worker count = %zu", _k->_engine->_conduit->getWorkerCount()); fflush_unlocked(stdout);

  // Updating solver's learning rate, if changed
  _optimizer->_eta = _learningRate;
  // Checking that incoming data has a correct format
  _problem->verifyData();

  // TODO maybe shuffle
  size_t N = _problem->_inputData.size();
  size_t T = _problem->_inputData[0].size();
  size_t IC = _problem->_inputData[0][0].size();
  size_t BS = _problem->_trainingBatchSize;
  size_t OC = _problem->_solutionData[0].size();
  // Remainder for unequal batch sizes
  size_t remainder = N % BS;
  // Iterations for epoch (without remainder)
  size_t IforE = N / BS;
  const auto nnHyperparameters = _neuralNetwork->getHyperparameters();
  std::vector<Sample> samples(IforE);
  // Sample Id and Batch Size
  size_t sId;
  for (sId = 0; sId < IforE; sId++)
  {
    // TODO make this maybe more efficient i.e. pass .data() address to overloaded getEvaluation
    // auto tBatch(_problem->_inputData.begin()+sId*sBS, _problem->_inputData.begin()+(sId+1)*sBS);
    // auto sBatch(_problem->_solutionData.begin()+sId*sBS, _problem->_solutionData.begin()+(sId+1)*sBS);
    // Carving part of the batch data that corresponds to this sample
    auto workerInputDataFlat = std::vector<float>(BS * T * IC);
    for (size_t i = 0; i < BS; i++)
      for (size_t j = 0; j < T; j++)
        for (size_t k = 0; k < IC; k++)
          workerInputDataFlat[i * T * IC + j * IC + k] = _problem->_inputData[sId * BS + i][j][k];
    auto workerSolutionDataFlat = std::vector<float>(BS * OC);
    for (size_t i = 0; i < BS; i++)
      for (size_t j = 0; j < OC; j++)
        workerSolutionDataFlat[i * OC + j] = _problem->_solutionData[sId * BS + i][j];
    // Grabbing constants
    samples[sId]["Sample Id"] = sId;
    samples[sId]["Module"] = "Solver";
    samples[sId]["Operation"] = "Run Iteration";
    samples[sId]["Input Data"] = workerInputDataFlat;
    samples[sId]["Input Dims"] = std::vector<size_t>({BS, T, IC});
    samples[sId]["Solution Data"] = workerSolutionDataFlat;
    samples[sId]["Solution Dims"] = std::vector<size_t>({BS, OC});
    samples[sId]["Hyperparameters"] = nnHyperparameters;
  }
  if(remainder){
    ++sId;
    auto workerInputDataFlat = std::vector<float>(remainder * T * IC);
    for (size_t i = 0; i < remainder; i++)
      for (size_t j = 0; j < T; j++)
        for (size_t k = 0; k < IC; k++)
          workerInputDataFlat[i * T * IC + j * IC + k] = _problem->_inputData[sId * remainder + i][j][k];
    auto workerSolutionDataFlat = std::vector<float>(remainder * OC);
    for (size_t i = 0; i < remainder; i++)
      for (size_t j = 0; j < OC; j++)
        workerSolutionDataFlat[i * OC + j] = _problem->_solutionData[sId * remainder + i][j];
    samples[sId]["Sample Id"] = sId;
    samples[sId]["Module"] = "Solver";
    samples[sId]["Operation"] = "Run Iteration";
    samples[sId]["Input Data"] = _problem->_inputData;
    samples[sId]["Input Dims"] = std::vector<size_t>({remainder, T, IC});
    samples[sId]["Solution Data"] = _problem->_solutionData;
    samples[sId]["Solution Dims"] = std::vector<size_t>({remainder, OC});
    samples[sId]["Hyperparameters"] = nnHyperparameters;
  }
  // Launching samples
  for (size_t i = 0; i < IforE; i++) KORALI_START(samples[i]);
  // Waiting for samples to finish
  KORALI_WAITALL(samples);

  //Averaging the gradients over the mini batches  //////////////////////////////
  _currentTrainingLoss = 0.0f;
  auto nnHyperparameterGradients = std::vector<float>(_neuralNetwork->_hyperparameterCount, 0.0f);
  for (size_t i = 0; i < IforE; i++) {
    _currentTrainingLoss += KORALI_GET(float, samples[i], "Loss");
    const auto grad = KORALI_GET(std::vector<float>, samples[i], "Hyperparameter Gradients");
    // Calculate the sum of the gradient batches/mean would only change the learning rate.
    std::transform(nnHyperparameterGradients.begin(), nnHyperparameterGradients.end(), grad.begin(), nnHyperparameterGradients.begin(), std::plus<float>());
  }
  // TODO maybe calculate rather the total loss and the average over the losses
  // TODO also the loss is currently calculated on not updates parameters
  // Calculate average traning loss
  _currentTrainingLoss = _currentTrainingLoss/(float) IforE;
  // raise(SIGINT);
  if(_validationSetData.size()){
    auto y_val = getEvaluation(_problem->_validationSetData);
    _currentValidationLoss = loss(y_val, _problem->_validationSetSolution);
    (*_k)["Results"]["Validation Loss"] = _currentValidationLoss;
  }
  (*_k)["Results"]["Training Loss"] = _currentTrainingLoss;

  // If the solution represents the gradients, just pass them on
  if (_lossFunction == "Direct Gradient") nnHyperparameterGradients = backwardGradients(_problem->_solutionData);
  // Passing hyperparameter gradients through a gradient descent update
  _optimizer->processResult(0.0f, nnHyperparameterGradients);
  // Getting new set of hyperparameters from the gradient descent algorithm
  _neuralNetwork->setHyperparameters(_optimizer->_currentValue);
}

float __className__::loss(const std::vector<std::vector<float>> &y, const std::vector<std::vector<float>> &yhat) const{
  auto N = y.size();
  auto OC = y[0].size();
  auto MSEVector = y;
  auto result = 0;
  for (size_t b = 0; b < N; b++)
    for (size_t i = 0; i < OC; i++)
      MSEVector[b][i] = y[b][i] - yhat[b][i];
  for (size_t b = 0; b < N; b++)
    for (size_t i = 0; i < OC; i++)
      result += MSEVector[b][i] * MSEVector[b][i];
  return result / ((float)N * 2.0f);
}

std::vector<std::vector<float>> __className__::dloss(std::vector<std::vector<float>> y, const std::vector<std::vector<float>> &yhat) const
{
  auto N = y.size();
  auto OC = y[0].size();
#pragma omp parallel for simd
  for (size_t b = 0; b < N; b++)
    for (size_t i = 0; i < OC; i++)
      y[b][i] = dloss(yhat[b][i], y[b][i]);
  return y;
}

float __className__::dloss(const float y, const float yhat) const {
  // TODO enable other losses
  return y-yhat;
}

std::vector<float> __className__::getHyperparameters()
{
  return _neuralNetwork->getHyperparameters();
}

void __className__::setHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  _neuralNetwork->setHyperparameters(hyperparameters);

  // Updating optimizer's current value
  _optimizer->_currentValue = hyperparameters;
}

std::vector<std::vector<float>> &__className__::getEvaluation(const std::vector<std::vector<std::vector<float>>> &input)
{
  // Grabbing constants
  const size_t N = input.size();

  // Running the input values through the neural network
  _neuralNetwork->forward(input);

  // Returning the output values for the last given timestep
  return _neuralNetwork->getOutputValues(N);
}

std::vector<float> __className__::backwardGradients(const std::vector<std::vector<float>> &gradients)
{
  // Grabbing constants
  const size_t N = gradients.size();

  // Running the input values through the neural network
  _neuralNetwork->backward(gradients);

  // Getting NN hyperparameter gradients
  auto hyperparameterGradients = _neuralNetwork->getHyperparameterGradients(N);

  // If required, apply L2 Normalization to the network's hyperparameters
  if (_l2RegularizationEnabled)
  {
    const auto nnHyperparameters = _neuralNetwork->getHyperparameters();
#pragma omp parallel for simd
    for (size_t i = 0; i < hyperparameterGradients.size(); i++)
      hyperparameterGradients[i] -= _l2RegularizationImportance * nnHyperparameters[i];
  }

  // Returning the hyperparameter gradients
  return hyperparameterGradients;
}

void __className__::runTrainingOnWorker(korali::Sample &sample)
{
  // Updating hyperparameters in the worker's NN
  auto nnHyperparameters = KORALI_GET(std::vector<float>, sample, "Hyperparameters");
  _neuralNetwork->setHyperparameters(nnHyperparameters);
  sample._js.getJson().erase("Hyperparameters");

  // Getting input from sample
  auto inputDataFlat = KORALI_GET(std::vector<float>, sample, "Input Data");
  sample._js.getJson().erase("Input Data");

  // Getting solution from sample
  auto solutionDataFlat = KORALI_GET(std::vector<float>, sample, "Solution Data");
  sample._js.getJson().erase("Solution Data");

  // Getting input dimensions
  auto inputDims = KORALI_GET(std::vector<size_t>, sample, "Input Dims");
  size_t N = inputDims[0];
  size_t T = inputDims[1];
  size_t IC = inputDims[2];
  sample._js.getJson().erase("Input Dims");

  // De-flattening input
  auto input = std::vector<std::vector<std::vector<float>>>(N, std::vector<std::vector<float>>(T, std::vector<float>(IC)));
  for (size_t i = 0; i < N; i++)
    for (size_t j = 0; j < T; j++)
      for (size_t k = 0; k < IC; k++)
        input[i][j][k] = inputDataFlat[i * T * IC + j * IC + k];

  // Getting solution dimensions
  auto solutionDims = KORALI_GET(std::vector<size_t>, sample, "Solution Dims");
  size_t OC = solutionDims[1];
  sample._js.getJson().erase("Solution Dims");

  // De-flattening solution
  auto solution = std::vector<std::vector<float>>(N, std::vector<float>(OC));
  for (size_t i = 0; i < N; i++)
    for (size_t j = 0; j < OC; j++)
      solution[i][j] = solutionDataFlat[i * OC + j];

  // Getting a reference to the neural network output
  const auto &results = getEvaluation(input);

  // Running Mean squared error function
  float squaredLoss = 0.0f;

// Calculating gradients via the loss function
#pragma omp parallel for simd
  for (size_t b = 0; b < N; b++)
    for (size_t i = 0; i < OC; i++)
      solution[b][i] = solution[b][i] - results[b][i];

  // Adding square losses
  for (size_t b = 0; b < N; b++)
    for (size_t i = 0; i < OC; i++)
      squaredLoss += solution[b][i] * solution[b][i];

  // Running the input values through the neural network
  backwardGradients(solution);

  // Storing the output values for the last given timestep
  sample["Hyperparameter Gradients"] = _neuralNetwork->getHyperparameterGradients(N);
  sample["Squared Loss"] = squaredLoss;
}

void __className__::runEvaluationOnWorker(korali::Sample &sample)
{
  // Updating hyperparameters in the worker's NN
  auto nnHyperparameters = KORALI_GET(std::vector<float>, sample, "Hyperparameters");
  _neuralNetwork->setHyperparameters(nnHyperparameters);
  sample._js.getJson().erase("Hyperparameters");

  // Getting input from sample
  auto inputDataFlat = KORALI_GET(std::vector<float>, sample, "Input Data");
  sample._js.getJson().erase("Input Data");

  // Getting input dimensions
  auto inputDims = KORALI_GET(std::vector<size_t>, sample, "Input Dims");
  size_t N = inputDims[0];
  size_t T = inputDims[1];
  size_t IC = inputDims[2];
  sample._js.getJson().erase("Input Dims");

  // De-flattening input
  auto input = std::vector<std::vector<std::vector<float>>>(N, std::vector<std::vector<float>>(T, std::vector<float>(IC)));
  for (size_t i = 0; i < N; i++)
    for (size_t j = 0; j < T; j++)
      for (size_t k = 0; k < IC; k++)
        input[i][j][k] = inputDataFlat[i * T * IC + j * IC + k];

  // Storing the output values for the last given timestep
  sample["Evaluation"] = getEvaluation(input);
}

void __className__::runIteration(korali::Sample &sample)
{
  // Updating hyperparameters in the worker's NN
  auto nnHyperparameters = KORALI_GET(std::vector<float>, sample, "Hyperparameters");
  _neuralNetwork->setHyperparameters(nnHyperparameters);
  sample._js.getJson().erase("Hyperparameters");

  // Getting input from sample
  auto inputDataFlat = KORALI_GET(std::vector<float>, sample, "Input Data");
  sample._js.getJson().erase("Input Data");

  // Getting solution from sample
  auto solutionDataFlat = KORALI_GET(std::vector<float>, sample, "Solution Data");
  sample._js.getJson().erase("Solution Data");

  // Getting input dimensions
  auto inputDims = KORALI_GET(std::vector<size_t>, sample, "Input Dims");
  size_t BS = inputDims[0];
  size_t T = inputDims[1];
  size_t IC = inputDims[2];
  sample._js.getJson().erase("Input Dims");

  // Getting solution dimensions
  auto solutionDims = KORALI_GET(std::vector<size_t>, sample, "Solution Dims");
  size_t OC = solutionDims[1];
  sample._js.getJson().erase("Solution Dims");

  // De-flattening input
  auto input = std::vector<std::vector<std::vector<float>>>(BS, std::vector<std::vector<float>>(T, std::vector<float>(IC)));
  for (size_t i = 0; i < BS; i++)
    for (size_t j = 0; j < T; j++)
      for (size_t k = 0; k < IC; k++)
        input[i][j][k] = inputDataFlat[i * T * IC + j * IC + k];

  // De-flattening solution
  auto y = std::vector<std::vector<float>>(BS, std::vector<float>(OC));
  for (size_t i = 0; i < BS; i++)
    for (size_t j = 0; j < OC; j++)
      y[i][j] = solutionDataFlat[i * OC + j];

  // FORWARD neural network on input data
  const auto yhat = getEvaluation(input);
  // TODO maybe add loss rather to problem than as part of learner ?
  // Calculate loss
  const auto batch_loss = loss(yhat, y);
  // Calculating gradients via the loss function
  const auto outputGrads = dloss(yhat, y);
  // BACKPROPAGATE the derivative of the output loss
  const auto hyperparameterGradients = backwardGradients(outputGrads);
  // TODO Regularization
  // If required, apply L2 Normalization to the network's hyperparameters
  // if (_l2RegularizationEnabled)
  // {
  //   const auto nnHyperparameters = _neuralNetwork->getHyperparameters();
  //   #pragma omp parallel for simd
  //   for (size_t i = 0; i < hyperparameterGradients.size(); i++)
  //       hyperparameterGradients[i] -= _l2RegularizationImportance * nnHyperparameters[i];
  // }
  // Storing the output values for the last given timestep
  sample["Hyperparameter Gradients"] = hyperparameterGradients;
  sample["Loss"] = batch_loss;
  // printf("Batch Loss %f batch_loss of sample\n", batch_loss); fflush(stdout);
}

void __className__::printGenerationBefore(){

}


void __className__::printGenerationAfter()
{
  if (_mode == "Training")
  {
    // Printing results so far
    size_t width = 60;
    char bar[width];
    size_t epoch = _k->_currentGeneration;
    _k->_logger->progressBar(epoch/(float)_maxGenerations, bar, width);
    _k->_logger->logInfo("Normal", "Epoch %zu/%zu %s Train Loss: %f | Val. Loss: %f \r", epoch, _maxGenerations, bar, _currentTrainingLoss, _currentValidationLoss);
    // if (_lossFunction == "Mean Squared Error") _k->_logger->logInfo("Normal", " + Training Loss: %.15f\n", _currentTrainingLoss);
    // if (_lossFunction == "Direct Gradient") _k->_logger->logInfo("Normal", "  + Gradient L2-Norm: %.15f\n", std::sqrt(_currentTrainingLoss));
  }
  if (_mode == "Testing")
  {
    // Printing results so far
    // if (_lossFunction == "Mean Squared Error") _k->_logger->logInfo("Normal", " + Testing Loss: %.15f", _currentTrainingLoss);
    // if (_lossFunction == "Direct Gradient") _k->_logger->logInfo("Normal", " + Gradient L2-Norm: %.15f", std::sqrt(_currentTrainingLoss));
  }
}

// TODO make templated function for multi size vectors
std::vector<float> __className__::flatten(const std::vector<std::vector<float>> &vec) const{
  auto N = vec.size();
  auto OC = vec[0].size();
  std::vector<float> vec_flat(N*OC);
  for (size_t i = 0; i < N; i++)
    for (size_t j = 0; j < OC; j++)
      vec_flat[i * OC + j] = vec[i][j];
}

std::vector<float> __className__::flatten(const std::vector<std::vector<std::vector<float>>> &vec) const{
  auto N = vec.size();
  auto T = vec[0].size();
  auto IC = vec[0].size();
  std::vector<float> vec_flat(N*IC);
  for (size_t i = 0; i < N; i++)
    for (size_t j = 0; j < T; j++)
      for (size_t k = 0; j < IC; j++)
        vec_flat[i * T * IC + j * IC + k] = vec[i][j][k];
}

__moduleAutoCode__;

__endNamespace__;
